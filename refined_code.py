# -*- coding: utf-8 -*-
"""Refined code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZxBhvwLz5CENLKkHseQUePgR2tTmnZ7Y
"""

# !pip install googletrans==4.0.0-rc1

import pandas as pd
import numpy as np
import torch
from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler
from transformers import BertTokenizerFast, BertForSequenceClassification, get_linear_schedule_with_warmup
from torch.optim import AdamW
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import random
import os
from googletrans import Translator, LANGUAGES

# ========================= REPRODUCIBILITY SEEDS =========================
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
set_seed(42)

# Commented out IPython magic to ensure Python compatibility.
# Mount Google Drive - applicable, if working on Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Set Working Directory - if working on Google Drive
# %cd /content/drive/MyDrive/COMP-702/Dissertation model

# =================== DATA LOADING & MERGING SECTION ===================
def load_and_merge_datasets():
    """
    Loads all news datasets, keeps only the title and label columns, and merges them.
    Ignores 'text' columns, uses only 'title' as input. Assumes all labels are already 0 (real) or 1 (fake).
    """
    dfs = []
    files_info = [
        'True.csv',
        'Fake.csv',
        'fake_or_real_news.csv',
        'test.csv',
        'evaluation.csv',
        'DataSet_Misinfo_TRUE.csv',
        'DataSet_Misinfo_FAKE.csv',
    ]
    for fname in files_info:
        df = None
        # Try several read options
        for sep in [',', ';']:
            for encoding in ['utf-8', 'latin1']:
                try:
                    df = pd.read_csv(fname, encoding=encoding, sep=sep, on_bad_lines='skip', low_memory=False)
                    # If it parses but has only one column, maybe wrong separator
                    if df.shape[1] <= 1:
                        df = None
                        continue
                    break
                except Exception as e:
                    df = None
            if df is not None:
                break
        if df is None:
            print(f"SKIPPING: {fname} (could not parse with any encoding/separator)")
            continue
        # Find title/label columns
        title_candidates = [c for c in df.columns if 'title' in c.lower()]
        label_candidates = [c for c in df.columns if 'label' in c.lower()]
        title_col = title_candidates[0] if title_candidates else df.columns[0]
        label_col = label_candidates[0] if label_candidates else 'label'
        if label_col not in df.columns:
            print(f"SKIPPING: {fname} (no label column found)")
            continue
        # Standardize columns: 'input' and 'label'
        df = df.rename(columns={title_col: 'input', label_col: 'label'})
        # Keep only 'input' and 'label' columns, and drop NaNs
        df = df[['input', 'label']].dropna()
        # Make sure label is integer and only keep 0/1
        try:
            df['label'] = df['label'].astype(int)
        except Exception as e:
            # If label is not already integer, try to map as string
            df['label'] = df['label'].map(lambda x: 1 if str(x).strip().lower() in ['1', 'fake', 'false'] else (0 if str(x).strip().lower() in ['0', 'true', 'real'] else np.nan))
            df = df.dropna(subset=['label'])
            df['label'] = df['label'].astype(int)
        df = df[df['label'].isin([0,1])]
        dfs.append(df)
    # Merge and shuffle
    if len(dfs) == 0:
        raise Exception("No valid datasets loaded!")
    data = pd.concat(dfs, ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)
    return data

data = load_and_merge_datasets()
print("Shape after merge:", data.shape)
print(data.head())

# ===================== DATA BALANCE VISUALIZATION ======================
plt.figure(figsize=(5,5))
plt.pie(
    [len(data[data.label==0]), len(data[data.label==1])],
    labels=['True', 'Fake'],
    autopct='%1.1f%%',
    explode=[0.05,0.05],
    colors=['skyblue', 'salmon']
)
plt.title("Label Distribution (0=Real, 1=Fake)")
plt.show()

# =========== TRAIN/VAL/TEST SPLIT (70/15/15) STRATIFIED ==============
train_text, temp_text, train_labels, temp_labels = train_test_split(
    data['input'], data['label'], test_size=0.3, random_state=42, stratify=data['label']
)
val_text, test_text, val_labels, test_labels = train_test_split(
    temp_text, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels
)

# Initialize translator object
translator = Translator()

def predict_news_multilingual(news_text):
    # Step 1: Translate any input to English
    try:
        translated = translator.translate(news_text, dest='en').text
    except Exception as e:
        print(f"Translation failed: {e}")
        # Fallback: Use the original text
        translated = news_text

    # Step 2: Tokenize as usual
    inputs = tokenizer(translated, padding=True, truncation=True, return_tensors="pt")
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # Step 3: Predict with your loaded model
    with torch.no_grad():
        logits = model(inputs['input_ids'], attention_mask=inputs['attention_mask']).logits
        pred = torch.argmax(logits, dim=1).item()
    return "Fake" if pred == 1 else "True"

# =================== TOKENIZATION SECTION ===================
MAX_LENGTH = 16  # use more context, set as high as VRAM allows
MODEL_NAME = 'bert-base-uncased'
tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)

def encode_texts(texts):
    return tokenizer.batch_encode_plus(
        list(texts),
        max_length=MAX_LENGTH,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )

tokens_train = encode_texts(train_text)
tokens_val = encode_texts(val_text)
tokens_test = encode_texts(test_text)

# ================ TORCH DATASET/LOADER SETUP =================
batch_size = 16  # Increase if you have lots of VRAM

train_data = TensorDataset(tokens_train['input_ids'], tokens_train['attention_mask'], torch.tensor(train_labels.values).long())
val_data   = TensorDataset(tokens_val['input_ids'], tokens_val['attention_mask'], torch.tensor(val_labels.values).long())
test_data  = TensorDataset(tokens_test['input_ids'], tokens_test['attention_mask'], torch.tensor(test_labels.values).long())

train_loader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size)
val_loader   = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=batch_size)
test_loader  = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=batch_size)

# =================== BERT MODEL SETUP ====================
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)
# Unfreeze ALL BERT layers for best fine-tuning
for param in model.bert.parameters():
    param.requires_grad = True
model = model.to(device)

optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)
epochs = 5  # you can set to 5–10 depending on patience/early stopping
total_steps = len(train_loader) * epochs
scheduler = get_linear_schedule_with_warmup(
    optimizer, num_warmup_steps=int(0.1*total_steps), num_training_steps=total_steps
)
loss_fn = torch.nn.CrossEntropyLoss()

# ================= EARLY STOPPING UTILITY ==================
class EarlyStopper:
    def __init__(self, patience=2, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.best_loss = float('inf')
        self.counter = 0

    def early_stop(self, val_loss):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            return False
        else:
            self.counter += 1
            return self.counter >= self.patience

# ===================== TRAINING LOOP =======================
train_losses, val_losses = [], []
early_stopper = EarlyStopper(patience=2)

for epoch in range(epochs):
    model.train()
    total_train_loss = 0
    for batch in train_loader:
        b_input_ids, b_attn_mask, b_labels = [b.to(device) for b in batch]
        model.zero_grad()
        outputs = model(b_input_ids, attention_mask=b_attn_mask, labels=b_labels)
        loss = outputs.loss
        total_train_loss += loss.item()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()
    avg_train_loss = total_train_loss / len(train_loader)
    train_losses.append(avg_train_loss)

    # ---- Validation ----
    model.eval()
    total_val_loss = 0
    preds, truths = [], []
    with torch.no_grad():
        for batch in val_loader:
            b_input_ids, b_attn_mask, b_labels = [b.to(device) for b in batch]
            outputs = model(b_input_ids, attention_mask=b_attn_mask, labels=b_labels)
            total_val_loss += outputs.loss.item()
            logits = outputs.logits
            preds += list(torch.argmax(logits, dim=1).cpu().numpy())
            truths += list(b_labels.cpu().numpy())
    avg_val_loss = total_val_loss / len(val_loader)
    val_losses.append(avg_val_loss)
    val_acc = accuracy_score(truths, preds)
    print(f"Epoch {epoch+1}: train_loss={avg_train_loss:.4f} val_loss={avg_val_loss:.4f} val_acc={val_acc:.4f}")

    # EARLY STOPPING
    if early_stopper.early_stop(avg_val_loss):
        print("Early stopping triggered.")
        break

# ======================= TEST SET EVALUATION =======================
model.eval()
test_preds, test_truths = [], []
with torch.no_grad():
    for batch in test_loader:
        b_input_ids, b_attn_mask, b_labels = [b.to(device) for b in batch]
        logits = model(b_input_ids, attention_mask=b_attn_mask).logits
        test_preds += list(torch.argmax(logits, dim=1).cpu().numpy())
        test_truths += list(b_labels.cpu().numpy())
print("\n==== TEST SET METRICS ====")
print(classification_report(test_truths, test_preds, target_names=['True','Fake']))
acc = accuracy_score(test_truths, test_preds)
print("Test Accuracy: %.2f%%" % (acc*100))

# === SAVE MODEL AND TOKENIZER FOR FUTURE USE (NO RETRAINING NEEDED) ===
import torch

# Save the trained model weights
model_save_path = "bert_fakenews_model.pt"
torch.save(model.state_dict(), model_save_path)

# Save the tokenizer (recommended with transformers)
tokenizer_save_path = "bert_fakenews_tokenizer"
tokenizer.save_pretrained(tokenizer_save_path)

print(f"Model weights saved to: {model_save_path}")
print(f"Tokenizer saved to: {tokenizer_save_path}")

# ======================= CONFUSION MATRIX ==========================
plt.figure(figsize=(6,6))
cm = confusion_matrix(test_truths, test_preds)
ConfusionMatrixDisplay(cm, display_labels=['Real/True','Fake']).plot(values_format='d', cmap='Blues')
plt.title('Confusion Matrix - Test Set')
plt.show()

# ============== UNSEEN NEWS HEADLINES PREDICTION ===============
print("\n==== Predicting on Unseen News Headlines ====")
unseen_news = [
    "Apple Announces New iPhone With Built-in Coffee Maker",                 # Fake
    "NASA Discovers New Habitable Planet in Distant Solar System",           # Fake
    "UNICEF Launches Global Campaign to Support Education in Conflict Zones",# True
    "World Leaders Announce Immediate Global Peace Agreement",               # Fake
    "Apple Unveils iPhone 15 With Improved Camera and Battery Life",         # True
    "Eating Chocolate Every Day Cures Common Cold, Scientists Claim", #Fake
    "NASA Finds 12-Foot Alien on Moon in Live Video Broadcast" ,# Fake
    "Drinking Bleach Recommended as Cure for COVID-19", #FAKE
    "Apple Releases New iPhone Model with Improved Camera Features", #TRUE
    "Olympic Committee Confirms Tokyo as Host City for Next Summer Games", #TRUE
    "苹果公司推出了带有内置咖啡机的新iPhone", #fake (chinese)
    "La NASA ha descubierto un nuevo planeta habitable.", #fake (spanish)
    "Apple ra mắt iPhone mới tích hợp máy pha cà phê" #fake (Vietnamese)
]
unseen_enc = encode_texts(unseen_news)
model.eval()
with torch.no_grad():
    logits = model(unseen_enc['input_ids'].to(device), attention_mask=unseen_enc['attention_mask'].to(device)).logits
    predictions = torch.argmax(logits, dim=1).cpu().numpy()
for text, pred in zip(unseen_news, predictions):
    print(f"News: {text}\nPredicted: {'Fake' if pred==1 else 'true'}\n")

# LOAD MODEL AND TOKENIZER FOR INFERENCE (NO NEED TO RETRAIN)
from transformers import BertTokenizer, BertForSequenceClassification
import torch

# Reload the tokenizer
tokenizer = BertTokenizer.from_pretrained("bert_fakenews_model.pt")

# Reload the model architecture and load weights
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
model.load_state_dict(torch.load("bert_fakenews_model.pt", map_location=torch.device('cpu')))  # or 'cuda'
model.eval()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

print("Model and tokenizer loaded. Ready for inference!")

print(LANGUAGES)