{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK8NlOvbTOpI"
      },
      "source": [
        "#ALL IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cu22wPs4TOpJ",
        "outputId": "2900df99-bbd5-4e0d-aa49-aa9413350388"
      },
      "outputs": [],
      "source": [
        "!pip install nlpaug nltk --quiet\n",
        "!pip install shap\n",
        "!pip install langdetect\n",
        "\n",
        "import os, nltk, certifi  # OS helpers, NLTK tools, and a trusted SSL cert bundle\n",
        "from pathlib import Path  # Clean, cross-platform file paths\n",
        "import pandas as pd  # Data frames for tables\n",
        "import numpy as np  # Fast arrays and math\n",
        "import torch  # PyTorch core\n",
        "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler, WeightedRandomSampler  # Batching and dataset helpers\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification, get_linear_schedule_with_warmup  # BERT tokenizer/model and LR scheduler\n",
        "from torch.optim import AdamW  # AdamW optimizer (weights decay friendly)\n",
        "from sklearn.model_selection import train_test_split  # Split data into train/validation/test\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay  # Evaluation metrics and confusion-matrix plot\n",
        "import matplotlib.pyplot as plt  # Charts and figures\n",
        "import random  # Simple random utilities (seeding)\n",
        "from langdetect import detect  # Detect the language of a text\n",
        "import shap  # Explain model predictions with SHAP values\n",
        "import nlpaug.augmenter.word as naw  # Word-level data augmentation\n",
        "from google.cloud import translate\n",
        "from google.oauth2 import service_account\n",
        "from google.auth.transport.requests import Request\n",
        "\n",
        "\n",
        "\n",
        "# Point SSL to a valid cert store to avoid HTTPS errors\n",
        "os.environ['SSL_CERT_FILE'] = certifi.where()\n",
        "PROJECT_ROOT = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
        "CANDIDATES = [\n",
        "    os.environ.get(\"GOOGLE_APPLICATION_CREDENTIALS\", \"\"),\n",
        "    PROJECT_ROOT / \"keys\" / \"translate-sa.json\",        # ./keys/translate-sa.json\n",
        "    PROJECT_ROOT.parent / \"keys\" / \"translate-sa.json\", # ../keys/translate-sa.json\n",
        "    Path.home() / \"gcp\" / \"translate-sa.json\",          # ~/gcp/translate-sa.json\n",
        "]\n",
        "key_path = next((Path(p).expanduser() for p in CANDIDATES if p and Path(p).expanduser().is_file()), None)\n",
        "if key_path:\n",
        "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = str(key_path)\n",
        "    creds = service_account.Credentials.from_service_account_file(str(key_path))\n",
        "    client = translate.TranslationServiceClient(credentials=creds)\n",
        "else:\n",
        "    os.environ.pop(\"GOOGLE_APPLICATION_CREDENTIALS\", None)\n",
        "    client = translate.TranslationServiceClient()\n",
        "# --- end auth ---\n",
        "nltk.data.path.clear()  # Reset NLTK’s search paths\n",
        "nltk.data.path.append('nltk_data')  # Tell NLTK to use the local ./nltk_data folder\n",
        "nltk.download('averaged_perceptron_tagger', download_dir='nltk_data')  # POS tagger model (Download a part-of-speech tagger)\n",
        "nltk.download('averaged_perceptron_tagger_eng', download_dir='nltk_data')  # English-only POS tagger\n",
        "nltk.download('wordnet', download_dir='nltk_data')  # WordNet lexical database (a dictionary of word relations)\n",
        "nltk.download('punkt', download_dir='nltk_data')  # Tokenizer models (a tokenizer to split text into sentences and words.)\n",
        "nltk.download('omw-1.4', download_dir='nltk_data')  # Multilingual WordNet data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pa2QW2xTOpK"
      },
      "source": [
        "# Setting up translation environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_9YdIS7TOpK",
        "outputId": "565630d0-822e-4492-9c6e-eda8ab51d32a"
      },
      "outputs": [],
      "source": [
        "# ----- portable credentials + project resolution -----\n",
        "SCOPES = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
        "LOCATION = os.getenv(\"TRANSLATE_LOCATION\", \"global\")  # override via env if needed\n",
        "\n",
        "PROJECT_ROOT = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
        "CANDIDATE_KEYS = [\n",
        "    os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\"),\n",
        "    PROJECT_ROOT / \"keys\" / \"translate-sa.json\",        # ./keys/translate-sa.json\n",
        "    PROJECT_ROOT.parent / \"keys\" / \"translate-sa.json\", # ../keys/translate-sa.json\n",
        "    Path.home() / \"gcp\" / \"translate-sa.json\",          # ~/gcp/translate-sa.json\n",
        "]\n",
        "\n",
        "def _find_key(cands):\n",
        "    for p in cands:\n",
        "        if not p:\n",
        "            continue\n",
        "        p = Path(p).expanduser()\n",
        "        if p.is_file():\n",
        "            return str(p)\n",
        "    return None\n",
        "\n",
        "KEY = _find_key(CANDIDATE_KEYS)\n",
        "\n",
        "if KEY:\n",
        "    creds = service_account.Credentials.from_service_account_file(KEY, scopes=SCOPES)\n",
        "    project_id = os.getenv(\"GOOGLE_CLOUD_PROJECT\") or os.getenv(\"GCP_PROJECT\") or creds.project_id\n",
        "else:\n",
        "    # Application Default Credentials (gcloud auth application-default login)\n",
        "    creds, project_id = google_auth_default(scopes=SCOPES)\n",
        "\n",
        "creds.refresh(Request())\n",
        "\n",
        "if not project_id:\n",
        "    raise RuntimeError(\n",
        "        \"No GCP project id resolved. Set GOOGLE_CLOUD_PROJECT or use a service account key with project_id.\"\n",
        "    )\n",
        "\n",
        "client = translate.TranslationServiceClient(credentials=creds)\n",
        "parent = f\"projects/{project_id}/locations/{LOCATION}\"\n",
        "# -----------------------------------------------------\n",
        "\n",
        "# sanity check\n",
        "resp = client.translate_text(request={\n",
        "    \"parent\": parent,\n",
        "    \"contents\": [\"Bonjour\"],\n",
        "    \"mime_type\": \"text/plain\",\n",
        "    \"target_language_code\": \"en\",\n",
        "})\n",
        "print(resp.translations[0].translated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SETTING SEED FOR REPRODUCIBILITY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loHKsD_ZE5q9"
      },
      "outputs": [],
      "source": [
        "# ========================= REPRODUCIBILITY SEEDS =========================\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVaeLjgNTOpK"
      },
      "source": [
        "# DATA LOADING & MERGING SECTION (FEEDBACK + MAIN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfgoOykXS5at",
        "outputId": "bc570f3c-480b-4a61-a73d-6c6f562bce77"
      },
      "outputs": [],
      "source": [
        "def load_and_merge_datasets():\n",
        "    import os, re, unicodedata\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from functools import lru_cache\n",
        "\n",
        "    # ---- regex helpers ----\n",
        "    AGENCY_RE   = re.compile(r\"\\b(reuters|associated\\s+press|ap|afp|xinhua|bbc|cnn|fox\\s+news|the\\s+cook\\s+political\\s+report|center\\s+for\\s+politics)\\b\", re.I)\n",
        "    URL_RE      = re.compile(r\"https?://\\S+|www\\.\\S+|t\\.co/\\S+|pic\\.twitter\\.com/\\S+\", re.I)\n",
        "    HANDLE_RE   = re.compile(r\"@\\w{2,}\", re.I)\n",
        "    HASHTAG_RE  = re.compile(r\"#\\w+\", re.I)\n",
        "    CREDIT_RE   = re.compile(r\"^\\s*(featured image via|photo by|image credit|via:)\\b.*$\", re.I|re.M)\n",
        "    LABEL_RE    = re.compile(r\"\\b(fake\\s+news|satire|rumou?r|hoax|fact[-\\s]?check(?:ed|ing)?)\\b\", re.I)\n",
        "\n",
        "    def _canon(s: str) -> str:\n",
        "        s = unicodedata.normalize(\"NFKC\", str(s)).lower()\n",
        "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "        return s\n",
        "\n",
        "    def _clean_text(s: str) -> str:\n",
        "        s = unicodedata.normalize(\"NFKC\", str(s))\n",
        "        s = URL_RE.sub(\" \", s)\n",
        "        s = HANDLE_RE.sub(\" \", s)\n",
        "        s = HASHTAG_RE.sub(\" \", s)\n",
        "        s = CREDIT_RE.sub(\" \", s)\n",
        "        s = AGENCY_RE.sub(\" [ORG] \", s)\n",
        "        s = LABEL_RE.sub(\" \", s)\n",
        "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "        return s\n",
        "\n",
        "    # ---- language detection + translation (sample-first) ----\n",
        "    BATCH = 128  # adjust to quota\n",
        "\n",
        "    def _looks_english(s: str) -> bool:\n",
        "        if not s:\n",
        "            return True\n",
        "        ascii_ratio = sum(1 for ch in s if ord(ch) < 128) / max(1, len(s))\n",
        "        return ascii_ratio >= 0.98 and any(v in s.lower() for v in \"aeiou\")\n",
        "\n",
        "    try:\n",
        "        from langdetect import detect\n",
        "        HAVE_DETECT = True\n",
        "    except Exception:\n",
        "        HAVE_DETECT = False\n",
        "\n",
        "    @lru_cache(maxsize=200_000)\n",
        "    def _lang(s: str) -> str:\n",
        "        if _looks_english(s):\n",
        "            return \"en\"\n",
        "        if HAVE_DETECT:\n",
        "            try:\n",
        "                return detect(s)\n",
        "            except Exception:\n",
        "                return \"en\"\n",
        "        return \"en\"\n",
        "\n",
        "    def _needs_translation(series: pd.Series, sample_n: int = 100) -> bool:\n",
        "        s = series.astype(str)\n",
        "        sample = s[s.str.strip().ne(\"\")].head(sample_n)\n",
        "        if sample.empty:\n",
        "            return False\n",
        "        # translate only if ANY sampled row is non-English\n",
        "        return any(_lang(text) != \"en\" for text in sample)\n",
        "\n",
        "    def _translate_batch(texts):\n",
        "        # requires configured `client` and `PARENT` for GCP Translate v3\n",
        "        try:\n",
        "            resp = client.translate_text(request={\n",
        "                \"parent\": PARENT,\n",
        "                \"contents\": texts,\n",
        "                \"mime_type\": \"text/plain\",\n",
        "                \"target_language_code\": \"en\",\n",
        "            })\n",
        "            return [t.translated_text for t in resp.translations]\n",
        "        except Exception:\n",
        "            return texts  # fail-open\n",
        "\n",
        "    def translate_series_to_en(series: pd.Series) -> pd.Series:\n",
        "        s = series.astype(str)\n",
        "        uniq = pd.Series(s.unique())\n",
        "        langs = uniq.map(_lang)\n",
        "        non_en_set = set(uniq[langs != \"en\"].tolist())\n",
        "        if not non_en_set:\n",
        "            return s\n",
        "        mask = s.isin(non_en_set)\n",
        "        out = s.copy()\n",
        "        idx = np.flatnonzero(mask.values)\n",
        "        for i in range(0, len(idx), BATCH):\n",
        "            j = idx[i:i+BATCH]\n",
        "            out.iloc[j] = _translate_batch(s.iloc[j].tolist())\n",
        "        return out\n",
        "\n",
        "    # ---- load files ----\n",
        "    dfs = []\n",
        "    DATASET_DIR = (Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()) / \"Datasets\"\n",
        "    if not DATASET_DIR.is_dir():\n",
        "        raise FileNotFoundError(f\"Dataset folder not found: {DATASET_DIR}\")\n",
        "\n",
        "    files_info = [\"True.csv\", \"Fake.csv\"]\n",
        "\n",
        "    for fname in files_info:\n",
        "        df = None\n",
        "        path = DATASET_DIR / fname\n",
        "        for sep in [\",\", \";\"]:\n",
        "            for encoding in [\"utf-8\", \"latin1\"]:\n",
        "                try:\n",
        "                    df = pd.read_csv(path, encoding=encoding, sep=sep, on_bad_lines=\"skip\", low_memory=False)\n",
        "                    if df.shape[1] <= 1:\n",
        "                        df = None\n",
        "                        continue\n",
        "                    break\n",
        "                except Exception:\n",
        "                    df = None\n",
        "            if df is not None:\n",
        "                break\n",
        "        if df is None:\n",
        "            print(f\"Skipping this file: {path.name} (could not parse)\")\n",
        "            continue\n",
        "\n",
        "        # find columns\n",
        "        cols = {c.lower(): c for c in df.columns}\n",
        "        title_col = next((cols[c] for c in cols if \"title\" in c), None)\n",
        "        text_col  = next((cols[c] for c in cols if \"text\" in c), None)\n",
        "        label_col = next((cols[c] for c in cols if \"label\" in c), None)\n",
        "\n",
        "        # build input\n",
        "        if title_col and text_col:\n",
        "            df['input'] = df[title_col].astype(str).fillna('') + ' [SEP] ' + df[text_col].astype(str).fillna('')\n",
        "        elif title_col:\n",
        "            df['input'] = df[title_col].astype(str).fillna('')\n",
        "        elif text_col:\n",
        "            df['input'] = df[text_col].astype(str).fillna('')\n",
        "        else:\n",
        "            print(f\"SKIPPING: {path.name} (no title or text column found)\")\n",
        "            continue\n",
        "\n",
        "        if label_col is None:\n",
        "            print(f\"SKIPPING: {path.name} (no label column found)\")\n",
        "            continue\n",
        "\n",
        "        df['label'] = df[label_col]\n",
        "\n",
        "        # translate only if the first 100 non-empty rows indicate non-English\n",
        "        if _needs_translation(df['input'], sample_n=100):\n",
        "            df['input'] = translate_series_to_en(df['input'])\n",
        "            print(f\"{path.name}: non-English detected in sample. Translated affected rows.\")\n",
        "        else:\n",
        "            print(f\"{path.name}: sample looks English. Skipping translation.\")\n",
        "\n",
        "        # clean and filter\n",
        "        df = df[['input', 'label']].dropna()\n",
        "        df['input_clean'] = df['input'].astype(str).apply(_clean_text)\n",
        "\n",
        "        valid_labels = {'0', '1', 'true', 'false', 'fake', 'real'}\n",
        "        df = df[df['label'].astype(str).str.strip().str.lower().isin(valid_labels)]\n",
        "\n",
        "        try:\n",
        "            df['label'] = df['label'].astype(int)\n",
        "        except Exception:\n",
        "            df['label'] = df['label'].map(\n",
        "                lambda x: 1 if str(x).strip().lower() in ['1', 'fake', 'false']\n",
        "                else (0 if str(x).strip().lower() in ['0', 'true', 'real'] else np.nan)\n",
        "            )\n",
        "            df = df.dropna(subset=['label'])\n",
        "            df['label'] = df['label'].astype(int)\n",
        "\n",
        "        df = df[df['label'].isin([0, 1])]\n",
        "        dfs.append(df)\n",
        "\n",
        "    if len(dfs) == 0:\n",
        "        raise Exception(\"No valid datasets loaded!\")\n",
        "\n",
        "    data = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # use cleaned text\n",
        "    data['input'] = data['input_clean']\n",
        "    data = data.drop(columns=['input_clean'])\n",
        "\n",
        "    initial_count = data.shape[0]\n",
        "\n",
        "    # duplicate handling\n",
        "    data['input'] = data['input'].str.strip()\n",
        "    data['input_lower'] = data['input'].str.lower()\n",
        "    dup_exact = data.duplicated('input_lower').sum()\n",
        "    data = data.drop_duplicates('input_lower')\n",
        "\n",
        "    data['canon'] = data['input'].apply(_canon)\n",
        "    dup_canon = data.duplicated('canon').sum()\n",
        "    data = data.drop_duplicates('canon').drop(columns=['canon', 'input_lower'])\n",
        "\n",
        "    final_count = data.shape[0]\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Initial rows before duplicate removal: {initial_count}\")\n",
        "    print(f\"Exact dups removed (raw): {dup_exact}\")\n",
        "    print(f\"Canonical dups removed (cleaned): {dup_canon}\")\n",
        "    print(f\"Rows left after removing duplicates: {final_count}\")\n",
        "    print(f\"Shape after merge and duplicate removal: {data.shape}\")\n",
        "    print(\"=\"*50)\n",
        "    print(data.head())\n",
        "\n",
        "    # save merged/cleaned\n",
        "    MAIN_DATA_PATH = \"Datasets/main_data.csv\"\n",
        "    data.to_csv(MAIN_DATA_PATH, index=False)\n",
        "    print(\"Merged data saved as main_data.csv\")\n",
        "\n",
        "    # ---- feedback integration ----\n",
        "    FEEDBACK_PATH = \"Datasets/user_feedback.csv\"\n",
        "    if os.path.exists(FEEDBACK_PATH):\n",
        "        print(\"=\"*50)\n",
        "        print(\"Feedback file found. Integrating corrections...\")\n",
        "        main_data = pd.read_csv(MAIN_DATA_PATH)\n",
        "        feedback = pd.read_csv(FEEDBACK_PATH)\n",
        "\n",
        "        if 'input' in feedback.columns:\n",
        "            if _needs_translation(feedback['input'], sample_n=100):\n",
        "                feedback['input'] = translate_series_to_en(feedback['input'])\n",
        "                print(\"Feedback: non-English detected in sample. Translated affected rows.\")\n",
        "            else:\n",
        "                print(\"Feedback: sample looks English. Skipping translation.\")\n",
        "            feedback.to_csv(FEEDBACK_PATH, index=False)\n",
        "\n",
        "        if feedback['label'].dtype.kind not in 'biufc':\n",
        "            label_map = {'Fake': 1, 'True': 0, 1: 1, 0: 0}\n",
        "            feedback['label'] = feedback['label'].map(label_map)\n",
        "\n",
        "        feedback = feedback[['input', 'label']].dropna()\n",
        "\n",
        "        main_data = main_data[~main_data['input'].isin(feedback['input'])]\n",
        "        updated_data = pd.concat([main_data, feedback], ignore_index=True)\n",
        "        updated_data = updated_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "        updated_data.to_csv(MAIN_DATA_PATH, index=False)\n",
        "        print(\"Feedback integrated and main_data.csv updated.\")\n",
        "        data = updated_data\n",
        "\n",
        "    # final shuffle for randomness\n",
        "    data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    return data\n",
        "\n",
        "\n",
        "# run\n",
        "data = load_and_merge_datasets()\n",
        "print(f\"after merging the feedback data and main data = {data.shape}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXwS0v1qTOpL"
      },
      "source": [
        "# Count current label distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7e6VLTHTpRK",
        "outputId": "71775bb1-0d36-457b-ac96-58d6c65cea56"
      },
      "outputs": [],
      "source": [
        "true_count = (data['label'] == 0).sum() # true label   \n",
        "fake_count = (data['label'] == 1).sum() # fake label\n",
        "\n",
        "print(f\"Before balancing: \\nTrue label news = {true_count} \\nFake label news = {fake_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXRjvdtfTOpL"
      },
      "source": [
        "# Choosing Augmenter: synonym replacement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxBnUt5sT0kp",
        "outputId": "5f5f74fe-807b-402d-b585-f6b1b0ad8aa7"
      },
      "outputs": [],
      "source": [
        "# # Autodetect and balance by augmenting the minority class with WordNet synonyms\n",
        "# aug = naw.SynonymAug(aug_src='wordnet', aug_max=10)\n",
        "\n",
        "# # Current class counts\n",
        "# counts = data['label'].value_counts()\n",
        "# if len(counts) != 2: # check if only 1 and 0 exists.\n",
        "#     raise ValueError(\"Expected binary labels 0/1. Got: \" + str(counts.to_dict()))\n",
        "\n",
        "# minority_label = counts.idxmin() # finds the label with the fewest rows.\n",
        "# majority_label = counts.idxmax() # finds the label with the most rows.\n",
        "\n",
        "# n_to_augment = counts[majority_label] - counts[minority_label] # calculate how many rows need to be augmented.\n",
        "# print(\"Detecting minority class....\")\n",
        "# if minority_label==1:\n",
        "#     print(f\"Minority class: Fake | 1 | Need to add: {n_to_augment}\")\n",
        "# else:\n",
        "#     print(f\"Minority class: True | 0 | Need to add: {n_to_augment}\")\n",
        "\n",
        "# if n_to_augment > 0: # only augment if the dataset is imbalanced.\n",
        "#     minority_df = data[data['label'] == minority_label].copy() # takes only the copy of minority rows.\n",
        "#     augmented_texts = []\n",
        "\n",
        "#     for i in range(n_to_augment): # loop upto difference in number of rows.\n",
        "#         original_text = minority_df.sample(1, random_state=42+i)['input'].values[0] # picks one random minority text each time, with a changing seed.\n",
        "#         new_text = aug.augment(original_text)           # may return str or list\n",
        "#         if isinstance(new_text, list): # if the augmenter returns a list, take the first string\n",
        "#             new_text = new_text[0]\n",
        "#         augmented_texts.append(new_text)\n",
        "\n",
        "#     aug_df = pd.DataFrame({'input': augmented_texts, 'label': minority_label}) # turns the new texts into a DataFrame with the minority label.\n",
        "#     data = pd.concat([data, aug_df], ignore_index=True) # adds the new rows to the dataset.\n",
        "#     data = data.sample(frac=1, random_state=42).reset_index(drop=True) # shuffles all rows and resets row numbers.\n",
        "# else:\n",
        "#     print(\"Already balanced. No augmentation applied.\")\n",
        "\n",
        "# print(\"After balancing:\", data['label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4bEDklWTOpL"
      },
      "source": [
        "# AFTER AUGMENTATION, NEW DATASET!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "YQyvKZNDS-VC",
        "outputId": "4f758fdf-c3eb-4ce4-9e70-4e16f8aff7a2"
      },
      "outputs": [],
      "source": [
        "# Visualize balance for your new, balanced dataset!\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.pie(\n",
        "    [len(data[data.label==0]), len(data[data.label==1])],\n",
        "    labels=['True', 'Fake'],\n",
        "    autopct='%1.1f%%',\n",
        "    explode=[0.05,0.05],\n",
        "    colors=['skyblue', 'salmon']\n",
        ")\n",
        "plt.title(\"Label Distribution After Augmentation (0=Real, 1=Fake)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWSt-SfDTOpL"
      },
      "source": [
        "# NORMALIZATION OF INPUT COLUMN\n",
        "### -LIST TO STRING\n",
        "### -REMOVE WHITE SPACE\n",
        "### -REMOVE \"NAN, NONE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w838hi2ZTOpL"
      },
      "outputs": [],
      "source": [
        "def normalize_input_cell(cell_value):\n",
        "    \"\"\"\n",
        "    - Cleans and standardizes a cell from the 'input' column:\n",
        "    - Flattens lists into a single string\n",
        "    - Removes None/nan/empty-like values\n",
        "    - Strips extra whitespace\n",
        "    \"\"\"\n",
        "\n",
        "    # If the cell contains a list, flatten and clean each element\n",
        "    if isinstance(cell_value, list):\n",
        "        cleaned_parts = [] # create a bucket for clean pieces.\n",
        "        for element in cell_value:\n",
        "            if element is None:\n",
        "                continue # Skip missing pieces.\n",
        "            element_str = str(element).strip() # Turn piece into text and trim spaces.\n",
        "            if element_str.lower() in (\"nan\", \"none\"):\n",
        "                continue\n",
        "            cleaned_parts.append(element_str)\n",
        "        return \" \".join(cleaned_parts).strip() # Join parts with a space. Trim ends.\n",
        "\n",
        "    # If the cell is None or a NaN float, return empty string\n",
        "    if cell_value is None or (isinstance(cell_value, float) and pd.isna(cell_value)):\n",
        "        return \"\"\n",
        "\n",
        "    # For normal strings (or other types), clean and return\n",
        "    cell_str = str(cell_value).strip()\n",
        "    return \"\" if cell_str.lower() == \"nan\" else cell_str\n",
        "\n",
        "\n",
        "# Apply cleaning to every cell in the 'input' column\n",
        "data[\"input\"] = data[\"input\"].apply(normalize_input_cell)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0jG5iyfTOpL"
      },
      "source": [
        "# TRAIN / VALIDATION / TEST SET SPLIT (70/15/15) STRATIFIED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5I32QLPATByi",
        "outputId": "e5821b0e-e204-4788-afb5-ac052a24c1fb"
      },
      "outputs": [],
      "source": [
        "# =========== TRAIN/VAL/TEST SPLIT (70/15/15) STRATIFIED ==============\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(\n",
        "    data['input'], data['label'], test_size=0.3, random_state=42, stratify=data['label']\n",
        ")\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(\n",
        "    temp_text, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
        ")\n",
        "\n",
        "# ---- Print basic stats ----\n",
        "print(f\"Training set:   {len(train_text)} samples\")\n",
        "print(f\"Validation set: {len(val_text)} samples\")\n",
        "print(f\"Test set:       {len(test_text)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0R1HPQeTOpM"
      },
      "source": [
        "# INITIALIZATION OF  TRANSLATOR OBJECT (GOOGLETRANS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8u9b3uL8v3z"
      },
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "PROJECT = \"cogent-metric-470213-i8\"\n",
        "LOCATION = \"global\"\n",
        "client = translate.TranslationServiceClient()\n",
        "PARENT = f\"projects/{PROJECT}/locations/{LOCATION}\"\n",
        "\n",
        "def gtranslate(text, dest=\"en\", src=None):\n",
        "    req = {\n",
        "        \"parent\": PARENT,\n",
        "        \"contents\": [text],\n",
        "        \"mime_type\": \"text/plain\",\n",
        "        \"target_language_code\": dest,\n",
        "    }\n",
        "    if src:  # e.g., \"fr\"\n",
        "        req[\"source_language_code\"] = src\n",
        "    resp = client.translate_text(request=req)\n",
        "    return SimpleNamespace(text=resp.translations[0].translated_text)\n",
        "\n",
        "def predict_news_multilingual(news_text):\n",
        "    try:\n",
        "        translated = gtranslate(news_text, dest=\"en\").text\n",
        "    except Exception:\n",
        "        translated = news_text\n",
        "    inputs = tokenizer(translated, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        logits = model(inputs['input_ids'], attention_mask=inputs['attention_mask']).logits\n",
        "        pred = torch.argmax(logits, dim=1).item()\n",
        "    return \"Fake\" if pred == 1 else \"True\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYs9NISETOpM"
      },
      "source": [
        "# TOKENIZATION SECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccg1qHRlTCbR"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 512  # use more context, set as high as VRAM allows\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def encode_texts(texts):\n",
        "    return tokenizer.batch_encode_plus( #Encode whole batch at once for consistency\n",
        "        list(texts),\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True, # add a mask of 1s for real tokens and 0s for padding.\n",
        "        return_tensors='pt' # return PyTorch tensors\n",
        "    )\n",
        "\n",
        "tokens_train = encode_texts(train_text) # tokenize the train split.\n",
        "tokens_val = encode_texts(val_text) # tokenize the validation split.\n",
        "tokens_test = encode_texts(test_text) # tokenize the test split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmalvlPBTOpM"
      },
      "source": [
        "# TORCH DATASET/LOADER SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_XFxepaTFgZ"
      },
      "outputs": [],
      "source": [
        "# batch_size = 64  # number of samples the model sees in one step.\n",
        "\n",
        "# train_data = TensorDataset(tokens_train['input_ids'], tokens_train['attention_mask'], torch.tensor(train_labels.values).long()) # a PyTorch dataset with three tensors: token ids, attention mask, and labels for training\n",
        "# val_data   = TensorDataset(tokens_val['input_ids'], tokens_val['attention_mask'], torch.tensor(val_labels.values).long())\n",
        "# test_data  = TensorDataset(tokens_test['input_ids'], tokens_test['attention_mask'], torch.tensor(test_labels.values).long())\n",
        "\n",
        "# train_loader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size) # creates an iterator that yields shuffled training batches.\n",
        "# val_loader   = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=batch_size) # no shuffle, stable and reproducible evaluation\n",
        "# test_loader  = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=batch_size) # no shuffle, stable and reproducible test\n",
        "\n",
        "batch_size = 64  # number of samples the model sees in one step.\n",
        "\n",
        "# build label tensors once\n",
        "y_train = torch.tensor(train_labels.values).long()\n",
        "y_val   = torch.tensor(val_labels.values).long()\n",
        "y_test  = torch.tensor(test_labels.values).long()\n",
        "\n",
        "train_data = TensorDataset(tokens_train['input_ids'], tokens_train['attention_mask'], y_train)\n",
        "val_data   = TensorDataset(tokens_val['input_ids'],   tokens_val['attention_mask'],   y_val)\n",
        "test_data  = TensorDataset(tokens_test['input_ids'],  tokens_test['attention_mask'],  y_test)\n",
        "\n",
        "# === Weighted sampler for class imbalance (replaces RandomSampler) ===\n",
        "class_counts  = torch.bincount(y_train)                 # e.g., tensor([n_true, n_fake])\n",
        "class_weights = (1.0 / class_counts.float())            # keep for loss_fn: CrossEntropyLoss(weight=class_weights.to(device))\n",
        "sample_weights = class_weights[y_train]                  # per-sample weights\n",
        "train_sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "# OLD: shuffled training loader (disabled)\n",
        "# train_loader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size)\n",
        "\n",
        "# NEW: weighted training loader\n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "val_loader   = DataLoader(val_data,   sampler=SequentialSampler(val_data),   batch_size=batch_size)\n",
        "test_loader  = DataLoader(test_data,  sampler=SequentialSampler(test_data),  batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk4wxwrWTOpM"
      },
      "source": [
        "# BERT (BASE) MODEL SETUP\n",
        "### UNFREEZING ALL LAYERS\n",
        "### ADAMW, WEIGHT DECAY, SCHEDULE WITH WARMUP, CROSS ENTROPY LOSS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "1e4de3c380934498bd01c89b986fe0ec",
            "41d442403c63428ba4a5e70881a51805",
            "977d2e3adb944d499e68834c73f5dcf2",
            "b2965199f54a44c6bcee5f141cce1309",
            "54aeb4e507954ebe9de6da3371ae4a42",
            "53285d6387c042a1b5d39b34adcb00ac",
            "27b79bcc61124176b6a1b89b50c9b36b",
            "5219e2cf2cf34ce69cde13624a3a351d",
            "09a8d9b9032847ccb587ee365676faff",
            "3d95eaf0109b4a4784b29affffa4a0b2",
            "38f39c959e324dfc96ec62bb583c9da4"
          ]
        },
        "id": "YxyIMDOFTNDN",
        "outputId": "105b6d8e-fcd8-465c-db9a-49df5b5efede"
      },
      "outputs": [],
      "source": [
        "# =================== BERT MODEL SETUP (top-3 layers unfrozen) ====================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "\n",
        "# Freeze everything\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Unfreeze top-3 encoder layers (9,10,11) + classifier\n",
        "for layer in model.bert.encoder.layer[-3:]:\n",
        "    for p in layer.parameters():\n",
        "        p.requires_grad = True\n",
        "for p in model.classifier.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "model = model.to(device)\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5, weight_decay=0.01)\n",
        "epochs = 5\n",
        "total_steps = len(train_loader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1*total_steps), num_training_steps=total_steps)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P38lwu1JTOpM"
      },
      "source": [
        "# SHAP EXPLAINATION FUNCTION SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UX48XM4NQPTf"
      },
      "outputs": [],
      "source": [
        "def explain_prediction(text, tokenizer, model, device, max_len=512):\n",
        "    \"\"\"\n",
        "    Generate SHAP explanations for a single text input.\n",
        "    Translates the model's prediction into per-token importance scores\n",
        "    showing how much each token contributed toward predicting 'Fake'.\n",
        "    Returns a list of (token, score) pairs.\n",
        "    \"\"\"\n",
        "    model.eval() # Switch the model to eval mode. Turns off dropout. Makes outputs stable\n",
        "    explainer = shap.Explainer(\n",
        "        lambda x: model(**tokenizer(list(x), padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\").to(device)).logits.softmax(-1).detach().cpu().numpy(), # Get the prediction probabilities\n",
        "        masker=shap.maskers.Text(tokenizer)\n",
        "    )\n",
        "    shap_values = explainer([text])\n",
        "    # shap_values.data[0] is the list of tokens, shap_values.values[0] is the list of SHAP values\n",
        "    tokens = shap_values.data[0]\n",
        "    scores = shap_values.values[0][:, 1]  # If 1 = Fake\n",
        "    # Return as a list of (token, score) pairs\n",
        "    return list(zip(tokens, scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1CE37dGTOpM"
      },
      "source": [
        "# EARLY STOPPING SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lejl7ztLEWmA"
      },
      "outputs": [],
      "source": [
        "class EarlyStopper:\n",
        "    \"\"\"It stops training when validation loss stops getting meaningfully better.\n",
        "    Tracks the best validation loss seen so far.\n",
        "    If the loss doesn’t improve by at least min_delta for patience checks, it tells you to stop.\"\"\"\n",
        "    def __init__(self, patience=2, min_delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "\n",
        "    def early_stop(self, val_loss):\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss # Saves the best loss record\n",
        "            self.counter = 0\n",
        "            return False # do not stop\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            return self.counter >= self.patience"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik8TckqlTOpM"
      },
      "source": [
        "# TRAINING LOOP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U9nRyRaTNqr",
        "outputId": "706a6879-eba5-4081-a0f7-1fc0ccb5e454"
      },
      "outputs": [],
      "source": [
        "train_losses, val_losses = [], []\n",
        "early_stopper = EarlyStopper(patience=2)\n",
        "best_val_loss = float('inf')  # track best validation loss.\n",
        "\n",
        "for epoch in range(epochs): # which is 10 right now.\n",
        "    model.train()           # Putting model into training mode. (Enables dropout and layer norms.)\n",
        "    total_train_loss = 0\n",
        "    for batch in train_loader:\n",
        "        b_input_ids, b_attn_mask, b_labels = [b.to(device) for b in batch] # Move inputs and labels to GPU or CPU. Needed for fast compute.\n",
        "        model.zero_grad()  # Clears old gradients from all model parameters so they don’t accumulate\n",
        "        outputs = model(b_input_ids, attention_mask=b_attn_mask, labels=b_labels) # Outputs are: Ids, Att. mask, label\n",
        "        loss = outputs.loss\n",
        "        total_train_loss += loss.item() # Add to the epoch total. For averaging later.\n",
        "        loss.backward() # Back propagation. Compute gradients.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Clip gradients to 1.0. Prevent exploding updates.\n",
        "        optimizer.step() # Apply gradients. Update weights.\n",
        "        scheduler.step() # Update learning rate. Warmup then decay.\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # ---- Validation ----\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    preds, truths = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            b_input_ids, b_attn_mask, b_labels = [b.to(device) for b in batch]\n",
        "            outputs = model(b_input_ids, attention_mask=b_attn_mask, labels=b_labels)\n",
        "            total_val_loss += outputs.loss.item()\n",
        "            logits = outputs.logits\n",
        "            preds += list(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "            truths += list(b_labels.cpu().numpy())\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_acc = accuracy_score(truths, preds)\n",
        "    print(f\"Epoch {epoch+1}: train_loss={avg_train_loss:.4f} val_loss={avg_val_loss:.4f} val_acc={val_acc:.4f}\")\n",
        "\n",
        "    # ---- SAVE BEST MODEL (overwrite existing folder) ----\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        model.save_pretrained(\"bert_fakenews_model\")\n",
        "        tokenizer.save_pretrained(\"bert_fakenews_model\")\n",
        "\n",
        "    # EARLY STOPPING\n",
        "    if early_stopper.early_stop(avg_val_loss):\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "# ======================= TEST SET EVALUATION =======================\n",
        "# Load the best model before testing\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert_fakenews_model\").to(device)\n",
        "\n",
        "model.eval()\n",
        "test_preds, test_truths = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        b_input_ids, b_attn_mask, b_labels = [b.to(device) for b in batch]\n",
        "        logits = model(b_input_ids, attention_mask=b_attn_mask).logits\n",
        "        test_preds += list(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        test_truths += list(b_labels.cpu().numpy())\n",
        "\n",
        "print(\"\\n==== TEST SET METRICS ====\")\n",
        "print(classification_report(test_truths, test_preds, target_names=['True','Fake']))\n",
        "acc = accuracy_score(test_truths, test_preds)\n",
        "print(\"Test Accuracy: %.2f%%\" % (acc*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6MoYmhbTOpN"
      },
      "source": [
        "# CONFUSION MATRIX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sC_aU1lHTW8h"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "cm = confusion_matrix(test_truths, test_preds)\n",
        "ConfusionMatrixDisplay(cm, display_labels=['Real/True','Fake']).plot(values_format='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix - Test Set')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-ZXBj6QTOpN"
      },
      "source": [
        "# UNSEEN NEWS HEADLINES PREDICTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0G_LDnfTOpN"
      },
      "outputs": [],
      "source": [
        "print(\"\\nPredicting on Unseen News Headlines\")\n",
        "# 0 = true / real   |   1 = fake / false\n",
        "unseen_news = [\n",
        "    \"The Chinese Academy of Sciences announces a new high-end quantum-computing lab in Shanghai, aiming for a 100-qubit breakthrough within five years.\",  # 0\n",
        "    \"The Chinese government will greatly increase rural-education funding this year to help more students in impoverished areas finish school.\",         # 0\n",
        "    \"Experts claim that eating spicy snack sticks every day can dramatically lower cancer risk and recommend a nationwide rollout.\",                    # 1\n",
        "    \"Beijing will offer free flu shots to all residents aged 65 + in the second half of the year to improve community health.\",                         # 0\n",
        "    \"WeChat officially announces that all money-transfer features will be permanently shut down for every user starting tomorrow.\",                     # 1\n",
        "    \"Scientists discover an alien creature over ten meters long in the Yangtze River, sparking nationwide debate.\",                                     # 1\n",
        "    \"The Shanghai government, together with several hospitals, launches a free COVID-19 booster-shot campaign.\",                                        # 0\n",
        "    \"The China Meteorological Administration predicts continuous snowfall across the entire country for three straight months this winter.\",            # 1\n",
        "    \"Huawei unveils the world’s first foldable 5G laptop, drawing industry attention.\",                                                                 # 0\n",
        "    \"A primary school in Shenzhen introduces a “Moon Survival Experience” course, where students will visit an actual moon base.\",                     # 1\n",
        "]\n",
        "\n",
        "\n",
        "unseen_enc = encode_texts(unseen_news)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(unseen_enc['input_ids'].to(device), attention_mask=unseen_enc['attention_mask'].to(device)).logits\n",
        "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "for text, pred in zip(unseen_news, predictions):\n",
        "    print(f\"News: {text}\\nPredicted: {'Fake' if pred==1 else 'true'}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09a8d9b9032847ccb587ee365676faff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e4de3c380934498bd01c89b986fe0ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41d442403c63428ba4a5e70881a51805",
              "IPY_MODEL_977d2e3adb944d499e68834c73f5dcf2",
              "IPY_MODEL_b2965199f54a44c6bcee5f141cce1309"
            ],
            "layout": "IPY_MODEL_54aeb4e507954ebe9de6da3371ae4a42"
          }
        },
        "27b79bcc61124176b6a1b89b50c9b36b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38f39c959e324dfc96ec62bb583c9da4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d95eaf0109b4a4784b29affffa4a0b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41d442403c63428ba4a5e70881a51805": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53285d6387c042a1b5d39b34adcb00ac",
            "placeholder": "​",
            "style": "IPY_MODEL_27b79bcc61124176b6a1b89b50c9b36b",
            "value": "model.safetensors: 100%"
          }
        },
        "5219e2cf2cf34ce69cde13624a3a351d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53285d6387c042a1b5d39b34adcb00ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54aeb4e507954ebe9de6da3371ae4a42": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "977d2e3adb944d499e68834c73f5dcf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5219e2cf2cf34ce69cde13624a3a351d",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09a8d9b9032847ccb587ee365676faff",
            "value": 440449768
          }
        },
        "b2965199f54a44c6bcee5f141cce1309": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d95eaf0109b4a4784b29affffa4a0b2",
            "placeholder": "​",
            "style": "IPY_MODEL_38f39c959e324dfc96ec62bb583c9da4",
            "value": " 440M/440M [00:01&lt;00:00, 504MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
